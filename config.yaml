# Configuration for KG_LFM training with lite dataset

seed: 42

dataset:
  lite: False
  base_path: "/leonardo_scratch/fast/IscrC_KG-LFM/dataset/Tri-Rex_V1"
  graph_embs_base_path: "/leonardo_scratch/fast/IscrC_KG-LFM/dataset/Tri-Rex_V1/graph_embs"
  preload_nodes_embeddings: False

pretrain_conf:
  run_name: "pretrain_trirex"
    
  epochs: 30
  early_stopping_patience: 3

  gradient_accumulation_steps: 16
  clip_grad_norm: 1.0

  learning_rate: 1e-4
  scheduler_eta_min: 1e-5
  weight_decay: 0.01
  
  checkpoint_dir: "/leonardo_work/IscrC_KG-LFM/checkpoints"
  checkpoint_frequency: 5
  
  resume : False

  dataloader:
    batch_size: 8
    shuffle: True
    num_workers: 0

    include_graphs: True
    return_tensors: "pt"

    pin_memory: true
    persistent_workers: true

model:
  # LLM Configuration
  llm_model_name: "Qwen/Qwen3-8B"
  
  # Graph Encoder Configuration
  graph_pooling: True
  dropout: 0.3  # Increased for better regularization
  num_heads: 4  # Reduced to save memory
  num_quantizers: 4  # Reduced to save memory
  codebook_size: 256  # Reduced to save memory
  shared_codebook: True
  
  # Training Configuration
  tune_language_model: False
  tune_kg_encoder: True
  kg_encoder_lr: 0.0001
  
  # Graph Node Embedding Configuration
  graph_nodes_embedding_model: "Qwen/Qwen3-Embedding-0.6B"